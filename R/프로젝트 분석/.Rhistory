xgb_model
# 단계 7: testset 생성
test_mat <- as.matrix(test[-c(5:6)])
dim(test_mat)
test_lab <- test$label
length(test_lab)
# 단계 8: model prediction
pred_iris <- predict(xgb_model, test_mat)
pred_iris
# 단계 9: confusion matrix
table(pred_iris, test_lab)
# 단계 10: 모델 성능평가1 - Accuracy
(19 + 13 + 12) / length(test_lab)
# 단계 11: model의 중요 변수(feature)와 영향력 보기
importance_matrix <- xgb.importance(colnames(train_mat),
model = xgb_model)
importance_matrix
# 단계 12: 중요 변수 시각화
xgb.plot.importance(importance_matrix)
# 실습: 간단한 인공신경망 모델 생성
# 단계 1: 패키지 설치
install.packages("nnet")
library(nnet)
# 단계 2: 데이터 셋 생성
df = data.frame(    # 데이터프레임 생성 - 입력 변수(x)와 출력변수(y)
x2 = c(1:6),
x1 = c(6:1),
y = factor(c('no', 'no', 'no', 'yes', 'yes', 'yes'))
)
str(df)
# 단계 3: 인공신경망 모델 생성
model_net = nnet(y ~ ., df, size = 1)
# 단계 4: 모델 결과 변수 보기
model_net
# 단계 5: 가중치(weights)보기
summary(model_net)
# 단계 6: 분류모델의 적합값 보기
model_net$fitted.values
# 단계 7: 분류모델의 예측치 생성과 분류 정확도
p <- predict(model_net, df, type = "class")
table(p, df$y)
# 실습: iris 데이터 셋을이용한 인공신경망 모델 생성
# 단계 1: 데이터 셋 생성
data(iris)
idx = sample(1:nrow(iris), 0.7 * nrow(iris))
training = iris[idx, ]
testing = iris[-idx, ]
nrow(training)
nrow(testing)
# 단계 2: 인공신경망 모델(은닉층 1개와 은닉층 3개) 생성
model_net_iris1 = nnet(Species ~ ., training, size = 1)
model_net_iris1
model_net_iris3 = nnet(Species ~ ., training, size = 3)
model_net_iris3
# 단계 3: 가중치 네트워크 보기 - 은닉층 1개 신경망 모델
summary(model_net_iris1)
# 단계 4:가중치 네트워크 보기 - 은닉층 3개 신경망 모델
summary(model_net_iris3)
# 단계 5: 분류모델 평가
table(predict(model_net_iris1, testing, type = "class"), testing$Species)
table(predict(model_net_iris3, testing, type = "class"), testing$Species)
# 실습: neuralnet 패키지를 이용한 인공신경망 모델 생성
# 단계 1: 패키지 설치
install.packages("neuralnet")
library(neuralnet)
# 단계 2: 데이터 셋 생성
data("iris")
idx = sample(1:nrow(iris), 0.7 * nrow(iris))
training_iris = iris[idx, ]
testing_iris = iris[-idx, ]
dim(training_iris)
dim(testing_iris)
# 단계 3: 수치형으로 칼럼 생성
training_iris$Species2[training_iris$Species == 'setosa'] <- 1
training_iris$Species2[training_iris$Species == 'versicolor'] <- 2
training_iris$Species2[training_iris$Species == 'virginica'] <- 3
training_iris$Species <- NULL
head(training_iris)
testing_iris$Species2[testing_iris$Species == 'setosa'] <- 1
testing_iris$Species2[testing_iris$Species == 'versicolor'] <- 2
testing_iris$Species2[testing_iris$Species == 'virginica'] <- 3
testing_iris$Species <- NULL
head(testing_iris)
# 단계 4: 데이터 정규화
# 단계 4-1: 정규화 함수 정의
normal <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
# 단계 4-2: 정규화 함수를 이용하여 학습데이터와/검정데이터 정규화
training_nor <- as.data.frame(lapply(training_iris, normal))
summary(training_nor)
# 단계 5: 인공신경망 모델 생성 - 은닉 노드 1개
model_net = neuralnet(Species2 ~ Sepal.Length + Sepal.Width +
Petal.Length + Petal.Width,
data = training_nor, hidden = 1)
model_net
plot(model_net)
# 단계 6: 분류모델 성능 평가
# 단계 6-1: 모델의 예측치 생성 - compute() 함수 이용
model_result <- compute(model_net, testing_nor[c(1:4)])
model_result$net.result
# 단계 6-2: 상관관계 분석 - 상관계수로 두 변수 간 선형관계의 강도 측정
cor(model_result$net.result, testing_nor$Species2)
# 단계 7: 분류모델 성능 향상 - 은닉층 노드 2개 지정, backprop 속성 적용
# 단계 7-1: 인공신경망 모델 생성
model_net2 = neuralnet(Species2 ~ Sepal.Length + Sepal.Width +
Petal.Length + Petal.Width,
data = training_nor, hidden = 2,
algorithm = "backprop", learningrate = 0.01)
# 단계 7-2: 분류모델 예측치 생성과 평가
model_result <- compute(model_net, testing_nor[c(1:4)])
cor(model_result$net.result, testing_nor$Species2)
install.packages("car")
install.packages("car")
# Chapter 17
# 실습: 비정상성 시계열을 정상성 시계열로 변경
# 단계 1: AirPassengers 데이터 셋 가져오기
data(AirPassengers)
# 단계 2: 차분 적용 - 평균 정상화
par(mfrow = c(1, 2))
ts.plot(AirPassengers)
diff <- diff(AirPassengers)
plot(diff)
# 단계 3: 로그 적용 - 분산 정상화화
par(mfrow = c(1, 2))
plot(AirPassengers)
log <- diff(log(AirPassengers))
plot(log)
# 실습: 단일 시계열 자료 시각화
# 단계 1: WWWusage 데이터 셋 가져오기
data("WWWusage")
str(WWWusage)
WWWusage
# 단계 2: 시계열 자료 추세선 시각화
# X11()
ts.plot(WWWusage, type = "l", col = "red")
# 실습: 다중 시계열 자료 시각화
# 단계 1: 데이터 가져오기
data(EuStockMarkets)
head(EuStockMarkets)
# 단계 2: 데이터프레임으로 변환
EuStock <- data.frame(EuStockMarkets)
head(EuStock)
# 단계 3: 단일 시계열 자료 추세선 시각화(1,000개 데이터 대상)
X11()
plot(EuStock$DAX[1:1000], type = "l", col = "red")
# 단계 4: 다중 시계열 자료 추세선 시각화(1,000개 데이터 대상)
plot.ts(cbind(EuStock$DAX[1:1000], EuStock$SMI[1:1000]),
main = "주가지수 추세선")
# 실습: 시계열 요소분해 시각화
# 단계 1: 시계열 자료 준비
data <- c(45, 56, 45, 43, 69, 75, 58, 59, 66, 64, 62, 65,
55, 49, 67, 55, 71, 78, 71, 65, 69, 43, 70, 75,
56, 56, 65, 55, 82, 85, 75, 77, 77, 69, 79, 89)
length(data)
# 단계 2: 시계열 자료 생성 - 시계열 자료 형식으로 객체 생성
tsdata <- ts(data, start = c(2016, 1), frequency = 12)
tsdata
# 단계 3: 추세선 확인 - 각 요인(추세, 순환, 계절, 불규칙)을 시각적으로 확인
ts.plot(tsdata)
# 단계 4: 시계열 분해
plot(stl(tsdata, "periodic"))
# 단계 5: 시계열 분해와 변동요인 제거
m <- decompose(tsdata)
attributes(m)
plot(m)
par(mfrow = c(1, 1))
plot(tsdata - m$seasonal)
# 단계 6: 추세요인과 불규칙요인 제거
plot(tsdata - m$trend)
plot(tsdata - m$seasonal - m$trend)
# 실습: 시계열 요소 분해 시각화
# 단계 1: 시계열 자료 생성
input <- c(3180, 3000, 3200, 3100, 3300, 3200,
3400, 3550, 3200, 3400, 3300, 3700)
length(input)
tsdata <- ts(input, start = c(2015, 2), frequency = 12)
# 단계2: 자기 상관 함수 시각화
acf(na.omit(tsdata), main ="자기상관함수", col = "red")
# 단계 3: 부분 자기 상관 함수 시각화
pacf(na.omit(tsdata), main = "부분 자기 상관 함수", col = "red")
# 실습: 시계열 자료의 추세 패턴 찾기 시각화
# 단계 1: 시계열 자료 생성
input <- c(3180, 3000, 3200, 3100, 3300, 3200,
3400, 3550, 3200, 3400, 3300, 3700)
# 단계 2: 추세선 시각화
plot(tsdata, type = "l", col = "red")
# 단계 3: 자기 상관 함수 시각화
acf(na.omit(tsdata), main = "자기 상환함수", col = "red")
# 단계 4: 차분 시각화
plot(diff(tsdata, differences = 1))
# 실습: 이동평균법을 이용한 평활하기
# 단계 1: 시계열 자료 생성
data <- c(45, 56, 45, 43, 69, 75, 58, 59, 66, 64, 62, 65,
55, 49, 67, 55, 71, 78, 71, 65, 69, 43, 70, 75,
56, 56, 65, 55, 82, 85, 75, 77, 77, 69, 79, 89)
length(data)
tsdata <- ts(data, start = c(2016, 1), frequency = 12)
tsdata
# 단계 2: 평화 ㄹ관련 패키지 설치
install.packages("TTR")
library(TTR)
# 단계 3: 이동평균법으로 평활 및 시각화
par(mfrow = c(2, 2))
plot(tsdata, main = "원 시계열 자료")
plot(SMA(tsdata, n = 1), main = "1년 단위 이동평균법으로 평활")
plot(SMA(tsdata, n = 2), main = "2년 단위 이동평균법으로 평활")
plot(SMA(tsdata, n = 3), main = "3년 단위 이동평균법으로 평활")
par(mfrow = c(1, 1))
# 실습: 계절성이 없는 정상성 시계열분석
# 단계 1: 시계열 자료 특성 분석
# 단계 1-1: 데이터 준비
input <- c(3180, 3000, 3200, 3100, 3300, 3200,
3400, 3550, 3200, 3400, 3300, 3700)
# 단계 1-2: 시계열 객체 생성(12개월: 2015 2월 ~ 2016년 1월)
tsdata <- ts(input, start = c(2015, 2), frequency = 12)
tsdata
# 단계 1-3: 추세선 시각화
plot(tsdata, type = "l", col = "red")
# 단계 2: 정상성 시계열 변환
par(mfrow = c(1, 2))
ts.plot(tsdata)
diff <- diff(tsdata)
plot(diff)
# 단계 3: 모형 식별과 추정
install.packages("forecast")
library(forecast)
arima <- auto.arima(tsdata)
arima
# 단계 4: 모형 생성
model <- arima(tsdata, order = c(1, 1, 0))
model
# 단계 5: 모형 진단(모형의 타당성 검정)
# 단계 5-1: 자기 상관 함수에 의한 모형 진단
tsdiag(model)
# 단계 5-2: Box_Ljung에 의한 잔차항 모형 진단
Box.test(model$residuals, lag = 1, type = "Ljung")
# 단계 5-2: Box_Ljung에 의한 잔차항 모형 진단
Box.test(model$residuals, lag = 1, type = "Ljung")
# 단계 6: 미래 예측(업무 적용)
fore <- forecast(model)
fore
par(mfrow = c(1, 2))
plot(fore)
model2 <- forecast(model, h = 6)
plot(model2)
# 실습: 계절성을 갖는 정상성 시계열분석
# 단계 1: 시계열 자로 특성 분석
# 단계 1-1: 데이터 준비
data <- c(55, 56, 45, 43, 69, 75, 58, 59, 66, 64, 62, 65,
55, 49, 67, 55, 71, 78, 61, 65, 69, 53, 70, 75,
56, 56, 65, 55, 68, 80, 65, 67, 77, 69, 79, 82,
57, 55, 63, 60, 68, 70, 58, 65, 70, 55, 65, 70)
length(data)
# 단계 1-2: 시계열 자료 생성
tsdata <- ts(data, start = c(2020, 1), frequency = 12)
tsdata
# 단게 1-3: 시계열 요소 분해 시각화
ts_feature <- stl(tsdata, s.window = "periodic")
plot(ts_feature)
# 단계 2: 정상성 시계열 변환
par(mfrow = c(1, 2))
ts.plot(tsdata)
diff <- diff(tsdata)
plot(diff)
# 단계 3: 모형 식별과 추정
library(forecast)
ts_model2 <- auto.arima(tsdata)
ts_model2
# 단계 4: 모형 생성
model <- arima(tsdata, c(0, 1, 1), seasonal = list(order = c(1, 1, 0)))
# Chapter 15
getwd()
# Chapter 15
getwd()
setwd("C:\Rwork\Lecture")
# Chapter 15
getwd()
setwd("C:\Rwork\R-script")
# Chapter 15
getwd()
# Chapter 15
getwd()
setwd("C:/Rwork/R-script")
# 실습: 단순 선형 회귀분석 수행
# 단계 1: 데이터 가져오기
product <- read.csv("C:/Rwork/Part-III/product.csv", header = TRUE)
str(product)
# 단계 2: 독립변수와 종속벼수 생성
y = product$제품_만족도
x = product$제품_적절성
df <- data.frame(x, y)
# 단계 3: 단순 선형회귀 모델 생성
result.lm <- lm(formula = y ~ x, data = df)
# 단계 4: 회귀분석의 절편과 기울기
result.lm
# 단계 5: 모델의 적합값과 잔차 보기
names(result.lm)
# 단계 5-1: 적합값 보기
fitted.values(result.lm)[1:2]
# 단계 5-2: 관측값 보기
head(df, 1)
# 단계 5-3: 회귀방정식을 적용하여 모델의 적합값 계산
Y = 0.7789 + 0.7393 * 4
Y
# 단계 5-4: 잔차(오차) 계산
3 - 3.735963
# 단계 5-5: 모델의 잔차 보기
residuals(result.lm)[1:2]
# 단계 5-6: 모델의 잔차와 회귀방정식에 의한 적합값으로부터 관측값 계산
-0.7359630 + 3.735963
# 실습: 선형 회귀분석 모델 시각화
# 단계 1: x, y 산점도 그리기
plot(formula = y ~ x, data = product)
# 단계 2: 선형 회귀모델 생성
result.lm <- lm(formula = y ~ x, data = product)
# 단계 5-1: 적합값 보기
fitted.values(result.lm)[1:2]
# 단계 5-2: 관측값 보기
head(df, 1)
# 단계 5-3: 회귀방정식을 적용하여 모델의 적합값 계산
Y = 0.7789 + 0.7393 * 4
Y
# 단계 5-4: 잔차(오차) 계산
3 - 3.735963
# 단계 5-5: 모델의 잔차 보기
residuals(result.lm)[1:2]
# 단계 5-6: 모델의 잔차와 회귀방정식에 의한 적합값으로부터 관측값 계산
-0.7359630 + 3.735963
# 실습: 선형 회귀분석 모델 시각화
# 단계 1: x, y 산점도 그리기
plot(formula = y ~ x, data = product)
# 단계 2: 선형 회귀모델 생성
result.lm <- lm(formula = y ~ x, data = product)
# 단계 2: 선형 회귀모델 생성
result.lm <- lm(formula = y ~ x, data = product)
# 단계 3: 회귀선
abline(result.lm, col = "red")
# Chapter 15
getwd()
# 실습: 단순 선형 회귀분석 수행
# 단계 1: 데이터 가져오기
product <- read.csv("C:/Rwork/Part-III/product.csv", header = TRUE)
str(product)
# 단계 2: 독립변수와 종속벼수 생성
y = product$제품_만족도
x = product$제품_적절성
df <- data.frame(x, y)
# 단계 3: 단순 선형회귀 모델 생성
result.lm <- lm(formula = y ~ x, data = df)
# 단계 4: 회귀분석의 절편과 기울기
result.lm
# 단계 5: 모델의 적합값과 잔차 보기
names(result.lm)
# 단계 5-1: 적합값 보기
fitted.values(result.lm)[1:2]
# 단계 5-2: 관측값 보기
head(df, 1)
# 단계 5-3: 회귀방정식을 적용하여 모델의 적합값 계산
Y = 0.7789 + 0.7393 * 4
Y
# 단계 5-4: 잔차(오차) 계산
3 - 3.735963
# 단계 5-5: 모델의 잔차 보기
residuals(result.lm)[1:2]
# 단계 5-6: 모델의 잔차와 회귀방정식에 의한 적합값으로부터 관측값 계산
-0.7359630 + 3.735963
# 실습: 선형 회귀분석 모델 시각화
# 단계 1: x, y 산점도 그리기
plot(formula = y ~ x, data = product)
# 단계 2: 선형 회귀모델 생성
result.lm <- lm(formula = y ~ x, data = product)
#R 내장 데이터 가져오기
data(iris)
#R 내장 데이터 가져오기
data(iris)
#iris 데이터 확인
str(iris)
#iris : 꽃받침, 꽃잎 데이터 추출
iris
names(iris1)
iris1 <- iris[, -5]
#기술통계량
summary(iris1)
#상관계수
cor(iris1, method="pearson")
#색의 농도로 상관계수
install.packages("corrgram")
library(corrgram)
corrgram(iris1, upper.panel = panel.conf)
#상관계수 챠트
install.packages("PerformanceAnalytics")
library(PerformanceAnalytics)
chart.Correlation(iris1)
#학습데이터와 테스트데이터 분리
1:nrow(iris1)
x <- sample(1:nrow(iris1), 0.7 * nrow(iris1))
train <- iris1[x, ]
test <- iris1[-x, ]
nrow(iris1)
nrow(test)
####회귀모델 : 꽃받침 길이 예측
names(iris1)
#학습
model1 <- lm(formula = Sepal.Length ~  Sepal.Width + Petal.Length + Petal.Width, data=train)
model2 <- lm(formula = Sepal.Length ~  Petal.Length + Petal.Width, data=train)
model3 <- lm(formula = Sepal.Length ~  Sepal.Width + Petal.Length, data=train)
model4 <- lm(formula = Sepal.Length ~  Sepal.Width + Petal.Width, data=train)
#예측
pred1 <- predict(model1, test)
pred2 <- predict(model2, test)
pred3 <- predict(model3, test)
pred4 <- predict(model4, test)
RMSE1 <- sqrt(mean((test$Sepal.Length - pred1)^2))
RMSE2 <- sqrt(mean((test$Sepal.Length - pred2)^2))
RMSE3 <- sqrt(mean((test$Sepal.Length - pred3)^2))
RMSE4 <- sqrt(mean((test$Sepal.Length - pred4)^2))
cat("예측1: ",RMSE1)
cat("예측2: ",RMSE2)
cat("예측3: ",RMSE3)
cat("예측4: ",RMSE4)
#분류모델
library(ggplot2)
ggplot(iris, aes(Sepal.Length, Sepal.Width))  +
geom_point(aes(colour = Species))
ggplot(iris, aes(Petal.Length , Petal.Width))  +
geom_point(aes(colour = Species))
#학습데이터와 테스트데이터 나누기
data(iris)
train <- iris[x, ]
test <- iris[-x, ]
# 단계 3: 오른쪽 item이 vegetables 단어가 포함된 규칙만 서브 셋으로 작성
oveg <- subset(rules, rhs %in% 'vegetables')
oveg
inspect(oveg)
# 단계 4: 왼쪽 item이 butter 또는 yogurt인 규칙만 서브 셋으로 작성
butter_yogurt <- subset(rules, lhs %in% c('butter', 'yogurt'))
butter_yogurt
inspect(butter_yogurt)
## 필요 칼럼만 호출
# 칼럼 보기
names(data)
## 경로 설정
getwd()
setwd("C:/Git/Data_analysis/R/프로젝트 분석")
## 군집분석(Clustering)을 위한 패키지 설치
install.packages("cluster")
library(cluster)
### 단계 1: 데이터셋 가져오기
data <- read.csv("상관분석R.csv", header = TRUE)
head(data)
## 필요 칼럼만 호출
# 칼럼 보기
names(data)
# 필요 칼럼만 호출
data <- data[c("평균운행속도", "최고속도") ]
# 데이터 확인
head(data)
# 단계 3: 비계층적 군집분석
result2 <- kmeans(data, 3)
# 단계 3: 비계층적 군집분석
result2 <- kmeans(data, 3)
# 데이터 확인
head(data)
## 필요 칼럼만 호출
# 칼럼 보기
names(data)
### 단계 1: 데이터셋 가져오기
data <- read.csv("상관분석R.csv", header = TRUE)
head(data)
# 필요 칼럼만 호출
data <- data[c("평균운행속도", "최고속도", "급감속건수") ]
# 데이터 확인
head(data)
### 단계 2: 유클리디안 거리 계산
idist <- dist(data)
head(idist)  ## 평균이 0인 데이터에 대해 오류가 발생
### 단계 3: 비계층적 군집분석
result <- kmeans(data, 3)
## 경로 설정
getwd()
setwd("C:/Git/Data_analysis/R/프로젝트 분석")
## 군집분석(Clustering)을 위한 패키지 설치
install.packages("cluster")
install.packages("cluster")
library(cluster)
### 단계 1: 데이터셋 가져오기
data <- read.csv("상관분석R.csv", header = TRUE)
head(data)
## 필요 칼럼만 호출
# 칼럼 보기
names(data)
# 필요 칼럼만 호출
data <- data[c("평균운행속도", "최고속도", "급감속건수") ]
# 데이터 확인
head(data)
### 단계 2: 유클리디안 거리 계산
idist <- dist(data)
head(idist)  ## 평균이 0인 데이터에 대해 오류가 발생
### 단계 3: 비계층적 군집분석
result <- kmeans(data, 3)
### 단계 3: 비계층적 군집분석
result <- kmeans(data, 3)
### 단계 3: 비계층적 군집분석
result <- kmeans(idist, 3)
names(result)
### 단계 3: 비계층적 군집분석
result <- kmeans(idist, 3)
### 단계 3: 비계층적 군집분석
result <- kmeans(data, 3)
### 단계 3: 비계층적 군집분석
result <- kmeans(data, 3)
